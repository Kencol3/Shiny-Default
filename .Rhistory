View(XDebt.Type.GDP.FILTER)
runApp('Desktop/Data Science Class/Debt')
runApp('Desktop/Data Science Class/Debt')
library(titanic3)
library(PASWR)
library('PASWR')
library(PASWR)
install.packages('PASWR')
library(PASWR)
data(titanic3)
titanic3
summary(titanic3)
dim(titanic3)
sapply(titanic3, sd)
sum(is.na(titanic3))
sapply(titanic3, sd) #age, fare, body are unable to perform stdev
library(mice) #Load the multivariate imputation by chained equations library.
library(lattice)
md.pattern(titainc3)
md.pattern(titanic3)
mice:: md.pattern(titanic3)
library(VIM)
VIM:aggr(sleep)
VIM::aggr(sleep) #A graphical interpretation of the missing values and their
VIM::aggr(sleep)
VIM::aggr(titanic3)
View(titanic3)
complete.cases(titanic3)
?complete.cases(titanic3)
ok <- complete.cases(titanic3)
sum(!ok)
1/1452
(1/1452)*100
(263/1452)*100
VIM::aggr(titanic3)
(1188/1452)*100
md.pattern(titanic3)
1452/ (1309*14)
###############################
#####Mean Value Imputation#####
###############################
#Creating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
?mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
?cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
#Mean value imputation method 1.
missing.data$x2[is.na(missing.data$x2)] = mean(missing.data$x2, na.rm=TRUE)
missing.data
#Mean value imputation method 2.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
missing.data = transform(missing.data, x2 = ifelse(is.na(x2),
mean(x2, na.rm=TRUE),
x2))
#Mean value imputation method 3.
library(caret)
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = caret::preProcess(missing.data, method = "medianImpute")
missing.data = predict(pre, missing.data)
missing.data
### Why Caret?
## 1. Maintain the structure of train - predict as other machine learning procedure.
##    This is particularly important when impute for future observation
## 2. Can be collected with other preprocesses, as below:
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("scale", "medianImpute"))
missing.data = predict(pre, missing.data)
missing.data
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
missing.data
pre = preProcess(missing.data, method = c("center","scale", "medianImpute"))
missing.data = predict(pre, missing.data)
missing.data
## manual scale
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
scaled = mapply(FUN = '/',missing.data,sapply(missing.data, function(x) {sd(x,na.rm=T)}))
scaled
## manual scale
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
scaled = mapply(FUN = '/',missing.data,sapply(missing.data, function(x) {sd(x,na.rm=T)}))
scaled
?mapply
scaled = mapply(FUN = '/',missing.data,sapply(missing.data, function(x) {sd(x,na.rm=T)}))
scaled
## manual scale
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
missing.data
scaled = mapply(FUN = '/',missing.data,sapply(missing.data, function(x) {sd(x,na.rm=T)}))
scaled
#Mean value imputation method 4.
library(Hmisc) #Load the Harrell miscellaneous library.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
imputed.x2 = impute(missing.data$x2, mean) #Specifically calling the x2 variable.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
##################################
#####Simple Random Imputation#####
##################################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
set.seed(0)
imputed.x2 = impute(missing.data$x2, "random") #Simple random imputation using the
imputed.x2 = impute(missing.data$x2, "random") #Simple random imputation using the
#impute() function from the Hmisc package.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
#############################
#####K-Nearest Neighbors#####
#############################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
imputed.1nn
#############################
#####K-Nearest Neighbors#####
#############################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
imputed.1nn
#Imputing using 5NN.
imputed.5nn = kNN(missing.data, k=5)
imputed.5nn
#Imputing using 9NN.
imputed.9nn = kNN(missing.data, k=9)
imputed.9nn
### Imputing with caret
### Note: knnImpute with caret::preProcess force normalization
#Imputing using 1NN.
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
imputed.1nn
#Imputing using 5NN.
pre.5nn = preProcess(missing.data, method = 'knnImpute', k=5)
imputed.5nn = predict(pre.5nn, missing.data)
imputed.5nn
#Imputing using 9NN.
pre.9nn = preProcess(missing.data, method = 'knnImpute', k=9)
imputed.9nn = predict(pre.9nn, missing.data)
imputed.1nn #Inspecting the imputed values of each of the methods;
imputed.5nn #what is going on here? Given our dataset, should we
imputed.9nn #expect these results?
#K-Nearest Neighbors regression on the sleep dataset.
sqrt(nrow(sleep)) #Determining K for the sleep dataset.
#Using 8 nearest neighbors.
pre.8nn = preProcess(sleep, method = 'knnImpute', k=8)
sleep.imputed8NN = predict(pre.8nn, sleep)
predict(pre.8nn, sleep)
summary(sleep) #Summary information for the original sleep dataset.
summary(sleep.imputed8NN[, 1:10]) #Summary information for the imputed sleep dataset.
#K-Nearest Neighbors regression on the sleep dataset.
sqrt(nrow(sleep)) #Determining K for the sleep dataset.
#Using 8 nearest neighbors.
pre.8nn = preProcess(sleep, method = 'knnImpute', k=8)
sleep.imputed8NN = predict(pre.8nn, sleep)
predict(pre.8nn, sleep)
summary(sleep) #Summary information for the original sleep dataset.
summary(sleep.imputed8NN[, 1:10]) #Summary information for the imputed sleep dataset.
#K-Nearest Neighbors classification on the iris dataset.
help(iris) #Inspecting the iris measurement dataset.
iris
iris.example = iris[, c(1, 2, 5)] #For illustration purposes, pulling only the
#Throwing some small amount of noise on top of the data for illustration
#purposes; some observations are on top of each other.
set.seed(0)
iris.example$Sepal.Length = jitter(iris.example$Sepal.Length, factor = .5)
iris.example$Sepal.Width = jitter(iris.example$Sepal.Width, factor= .5)
col.vec = c(rep("red", 50), #Creating a color vector for plotting purposes.
rep("green", 50),
rep("blue", 50))
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
?jitter()
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
missing.vector = c(41:50, 91:100, 141:150) #Inducing missing values on the Species
iris.example$Species[missing.vector] = NA  #vector for each category.
iris.example
col.vec[missing.vector] = "purple" #Creating a new color vector to
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica", "NA"),
pch = 16, col = c("red", "green", "blue", "purple"), cex = .75)
#Inspecting the Voronoi tesselation for the complete observations in the iris
#dataset.
library(deldir) #Load the Delaunay triangulation and Dirichelet tesselation library.
info = deldir(iris.example$Sepal.Length[-missing.vector],
iris.example$Sepal.Width[-missing.vector])
plot.tile.list(tile.list(info),
fillcol = col.vec[-missing.vector],
main = "Iris Voronoi Tessellation\nDecision Boundaries")
#Adding the observations that are missing species information.
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = 16, col = "white")
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = "?", cex = .66)
#Conducting a 1NN classification imputation.
iris.imputed1NN = kNN(iris.example, k = 1)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed1NN$Species)
#Conducting a 12NN classification imputation based on the square root of n.
sqrt(nrow(iris.example))
iris.imputed12NN = kNN(iris.example, k = 12)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed12NN$Species)
##################################################
#####Using Minkowski Distance Measures in KNN#####
##################################################
library(kknn) #Load the weighted knn library.
#Separating the complete and missing observations for use in the kknn() function.
complete = iris.example[-missing.vector, ]
missing = iris.example[missing.vector, -3]
#Distance corresponds to the Minkowski power.
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
summary(iris.euclidean)
##################################################
#####Using Minkowski Distance Measures in KNN#####
##################################################
library(kknn) #Load the weighted knn library.
#Separating the complete and missing observations for use in the kknn() function.
complete = iris.example[-missing.vector, ]
missing = iris.example[missing.vector, -3]
#Distance corresponds to the Minkowski power.
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
summary(iris.euclidean)
iris.manhattan = kknn(Species ~ ., complete, missing, k = 12, distance = 1)
summary(iris.manhattan)
nrow(iris.manhattan)
iris.manhattan
getwd()
setwd('StorEDGE/Blackstone/')
setwd('~/StorEDGE/Blackstone/')
setwd('StorEDGE/Blackstone/')
setwd('~/Downloads')
Default = read.csv('CTDefault.csv')
View(Default)
Paris = read.csv('ParisClub.csv')
Paris = read.csv('ParisClub.csv')
View(Paris)
Paris = read.csv('ParisClub.csv')
View(Paris)
Paris[0]
Paris[:]
Paris[:,:]
Paris[:,0]
Paris[-1]
Paris[-c('X','X.1']
Paris = as.data.frame(Paris)
Paris[-c('X')]
Paris['X']
Paris[-'X']
Paris[0:2]
Paris[0:7]
Paris[0:6]
Paris = Paris[0:6]
View(Paris)
View(Default)
levels(Default)
class(Default)
Default = as.data.frame(Default)
names(Default)
library(dplyr)
Default <- Default %>% rename(Underlying.Discount.Rate = 'Underlying.Discount.............Rate',
Debt.Restructured.mUSD = 'Debt.Restructured....m.US..',
Market.Haircut.HM = 'Market.Haircut........HM',
Face.Value.Reduction.Pct = 'Face.Value.Reduction..in...',
Reduction.Face.Value = 'Reduct..in.Face.Value',
Country.Raw = 'Country...Case')
names(Default)
names(Paris)
Paris <- Paris %>% rename( Debt.Affected.mUSD = 'Debt.Affected..m.US..',
HIPC.Debt.Relief = 'Part.of.HIPC.Debt.Relief.',
Reduction.of.Face.Value = 'Reduction.of.Face.Value.')
View(Paris)
#####################################################
#####################################################
#####[06] Generalized Linear Models Lecture Code#####
#####################################################
#####################################################
GradSchool <- fread('Graduate_Schools.txt', stringsAsFactors = FALSE)
#####################################################
#####################################################
#####[06] Generalized Linear Models Lecture Code#####
#####################################################
#####################################################
library(data.table)
GradSchool <- fread('Graduate_Schools.txt', stringsAsFactors = FALSE)
View(GradSchool)
GradSchool <- Gradschool[1:4]
GradSchool <- GradSchool[1:4]
colnames(GradSchool) = c('admit', 'gre', 'gpa', 'rank')
GradSchool <- GradSchool[1:4]
colnames(GradSchool) = c('admit', 'gre', 'gpa', 'rank')
View(GradSchool)
GradSchool <- fread('Graduate_Schools.txt', stringsAsFactors = FALSE)
GradSchool <- GradSchool[,1:4]
colnames(GradSchool) = c('admit', 'gre', 'gpa', 'rank')
View(GradSchool)
GradSchool <- fread('Graduate_Schools.txt', stringsAsFactors = FALSE)
GradSchool <- GradSchool[,1:5]
GradSchool <- GradSchool[,2:5]
colnames(GradSchool) = c('admit', 'gre', 'gpa', 'rank')
GradSchools <- fread('Graduate_Schools.txt', stringsAsFactors = FALSE)
GradSchools <- GradSchools[,2:5]
colnames(GradSchools) = c('admit', 'gre', 'gpa', 'rank')
head(GradSchools)
summary(GradSchools) #Looking at the five number summary information.
sapply(GradSchools, sd) #Looking at the individual standard deviations.
sapply(GradSchools, class) #Looking at the variable classes.
table(GradSchools$admit)/nrow(GradSchools) #Manually calculating the proportions.
table(GradSchools$admit, GradSchools$rank) #Checking to see that we have data
plot(GradSchools, col = GradSchools$admit + 2) #Basic graphical EDA.
GradSchools$rank = as.factor(GradSchools$rank) #Converting the rank variable to
#Being naïve at first and fitting a multiple linear regression model.
bad.model = lm(admit ~ gre + gpa + rank, data = GradSchools)
summary(bad.model) #Looks like everything is significant, so what's bad?
plot(bad.model) #Severe violations to the assumptions of linear regression.
logit.overall = glm(admit ~ gre + gpa + rank,
family = "binomial",
data = GradSchools)
#Residual plot for logistic regression with an added loess smoother; we would
#hope that, on average, the residual values are 0.
scatter.smooth(logit.overall$fit,
residuals(logit.overall, type = "deviance"),
lpars = list(col = "red"),
xlab = "Fitted Probabilities",
ylab = "Deviance Residual Values",
main = "Residual Plot for\nLogistic Regression of Admission Data")
abline(h = 0, lty = 2)
library(car)
influencePlot(logit.overall) #Can still inspect the influence plot.
summary(logit.overall) #Investigating the overall fit of the model.
exp(logit.overall$coefficients)
#Inspecting the relationship between log odds and odds.
cbind("Log Odds" = logit.overall$coefficients,
"Odds" = exp(logit.overall$coefficients))
confint(logit.overall) #For logistic regression objects, the confint() function
confint.default(logit.overall) #To generate confidence intervals for logistic
#Generating confidence intervals for the coefficients on the odds scale.
exp(confint(logit.overall))
exp(confint.default(logit.overall))
#Do the categories for rank add any predictive power to the model? Let's
#conduct the drop in deviance test:
logit.norank = glm(admit ~ gre + gpa,
family = "binomial",
data = GradSchools)
reduced.deviance = logit.norank$deviance #Comparing the deviance of the reduced
reduced.df = logit.norank$df.residual    #model (the one without rank) to...
full.deviance = logit.overall$deviance #...the deviance of the full model (the
full.df = logit.overall$df.residual    #one with the rank terms).
pchisq(reduced.deviance - full.deviance,
reduced.df - full.df,
lower.tail = FALSE)
#More simply, we can use the anova() function and set the test to "Chisq".
anova(logit.norank, logit.overall, test = "Chisq")
#How does the probability of admission change across ranks for a student
#who has an average GRE and an average GPA?
newdata = with(GradSchools, data.frame(gre = mean(gre),
gpa = mean(gpa),
rank = factor(1:4)))
newdata #Creating a data frame with the average GRE and GPA for each level of
predict(logit.overall, newdata) #This gives us the log odds; but we want
#Using the formula to convert to probabilities:
exp(predict(logit.overall, newdata))/(1 + exp(predict(logit.overall, newdata)))
#Setting the type to "response" converts the predictions to probabilities for
#us automatically:
predict(logit.overall, newdata, type = "response")
#Making it easier to see the effects of the rank variable by printing out the
#results side-by-side:
cbind(newdata, "P_Admit" = predict(logit.overall, newdata, type = "response"))
#Converting the fitted probabilities to binary:
admitted.predicted = round(logit.overall$fitted.values)
#Comparing the true values to the predicted values:
table(truth = GradSchools$admit, prediction = admitted.predicted)
#It seems like this model made a lot of mistakes (116 out of 400)! This is quite
#dreadful in this case. Let's do a little bit more exploring. We never looked at
#the overall test of deviance:
pchisq(logit.overall$deviance, logit.overall$df.residual, lower.tail = FALSE)
#What about checking the McFadden's pseudo R^2 based on the deviance?
1 - logit.overall$deviance/logit.overall$null.deviance
#What have we found out? The overall model we created doesn't give us much
#predictive power in determining whether a student will be admitted to
#graduate school.
table(GradSchools$admit) #Our data contains 273 unadmitted students and 127
#admitted students.
table(admitted.predicted) #The model we created predicts that 351 students will
#not be admitted, and only 49 will be admitted.
table(truth = GradSchools$admit, prediction = admitted.predicted)
library(data.table)
GradSchools <- fread('Graduate_Schools.txt', stringsAsFactors = FALSE)
View(GradSchools)
GradSchools <- GradSchools[,2:5]
colnames(GradSchools) = c('admit', 'gre', 'gpa', 'rank')
g <- ggplot(Default, aes(Date, `Preferred Haircut HSZ`)) +
labs(title = 'Sovereign Defaults', subtitle = '1970 to 2014') +
geom_jitter(aes(col=region, size = `Debt Restructured (USDmn)`))
setwd('~/Desktop/Shiny-Default')
library(dplyr)
library(shiny)
library(data.table)
library(wbstats)
library(tidyr)
library(googleVis)
library(ggplot2)
countries <- data.table(wbcountries())
ISO <- countries[,0:3]
RegInc <- countries[,c('iso2c','region','income','lending')]
View(countries)
Default <- fread('CTDefault.csv', stringsAsFactors = FALSE)
Default <- as.data.frame(Default)
Default <- merge(Default, ISO, by.x = 'Code', by.y = 'iso3c')
Default <- Default[,c(2,20,21,4:19,3,1)]
Default <- merge(Default, RegInc, by = 'iso2c')
Default[Default == 'none (buy back)'] <- NA
Default <- separate(Default, Date, into = c('Month', 'Year'), sep = '-')
Default <- Default %>% mutate(Year = ifelse(Year <70,
paste('20', Year, sep=''),
paste('19', Year, sep='')))
Default <- unite(Default, 'Date', Month, Year, sep = '-01-')
Default$Date <- as.Date(Default$Date, '%b-%d-%Y')
class(Default$Date)
Default$`Preferred Haircut HSZ` <- as.numeric(sub('%','',Default$`Preferred Haircut HSZ`))
Default$`Underlying Discount Rate` <- as.numeric(sub('%','',Default$`Underlying Discount Rate`))
Default$`Market Haircut HM` <- as.numeric(sub('%','',Default$`Market Haircut HM`))
Default$`Face Value Reduction (in %)` <- as.numeric(sub('%','',Default$`Face Value Reduction (in %)`))
Default$`Debt Restructured (USDmn)` <- as.numeric(sub(',','',Default$`Debt Restructured (USDmn)`))
Default$`Debt Restructured (USDmn)` <- Default$`Debt Restructured (USDmn)`/1000
g <- ggplot(Default, aes(Date, `Preferred Haircut HSZ`)) +
labs(title = 'Sovereign Defaults', subtitle = '1970 to 2014') +
geom_jitter(aes(col=region, size = `Debt Restructured (USDmn)`))
plot(g)
states = as.data.frame(state.x77) 
colnames(states)[4] = "Life.Exp"
colnames(states)[6] = "HS.Grad"
states[,9] = (states$Population*1000)/states$Area
colnames(states)[9] = "Density"
fit = lm(Life.Exp ~ ., data = states)
y = states$Life.Exp
X = model.matrix(Life.Exp ~ ., data = states)
n = dim(X)[1]
p = dim(X)[2]-1
residuals = y - X %*% fit$coefficients
RSS = sum((fit$residuals)^2)
RSE = sqrt(RSS/fit$df.residual)
TSS = sum((y-mean(y))^2)
F_stat = ((TSS-RSS)/p) / (RSS/(n-p-1))
pf(F_stat,p,n-p-1,lower.tail = F)
RSq = 1-RSS/TSS
multRSq = 1 - (RSS/(n-p-1)) / (TSS/(n-1))
VCV = vcov(fit)
SDs = sqrt(diag(VCV))
SDs
(summary(fit))$coefficients[,2]
VCV
b = solve(t(X)%*%X) %*% t(X) %*% y
b
(summary(fit))$coefficients[,1]
t = b / SDs
t
# 1 Residuals vs Fitted
# Do the residuals evenly disburse around 0?
# Are there no patterns?
plot(fit,which = 1)
plot(fit$fitted.values,fit$residuals)
# 2 Normal QQ
# Do the points follow a straight line?
plot(fit,which = 2)
plot(qnorm((1:n)/n - (.5/n)),sort(fit$residuals)/sd(fit$residuals))
abline(0,1)
# 3 Scale-Location
# Is there a pattern in variance?
plot(fit,which = 3)
plot(fit$fitted.values,rstandard(fit))
# 4 Residuals vs Leverage
plot(fit,which = 5)
plot(hat(X),sort(fit$residuals)/sd(fit$residuals))
install.packages('dummy')
library(dummy)
#create toy data
(traindata <- data.frame(var1=as.factor(c("a","b","b","c")),
var2=as.factor(c(1,1,2,3)),
var3=c("val1","val2","val3","val3"),
stringsAsFactors=FALSE))
(dummies_train <- dummy(x=traindata))
